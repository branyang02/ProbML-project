{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CACHE_DIR = f\"/scratch/{os.getenv('USER')}/huggingface_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect LM hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8299e4d72f4cc8a34335d9a56cea33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.text_generation import TextGenerationPipeline, ReturnType\n",
    "import numpy as np\n",
    "\n",
    "class CustomPipeline(TextGenerationPipeline):\n",
    "    \n",
    "    def _forward(self, model_inputs, **generate_kwargs):\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "        # Allow empty prompts\n",
    "        if input_ids.shape[1] == 0:\n",
    "            input_ids = None\n",
    "            attention_mask = None\n",
    "            in_b = 1\n",
    "        else:\n",
    "            in_b = input_ids.shape[0]\n",
    "        prompt_text = model_inputs.pop(\"prompt_text\")\n",
    "\n",
    "        # If there is a prefix, we may need to adjust the generation length. Do so without permanently modifying\n",
    "        # generate_kwargs, as some of the parameterization may come from the initialization of the pipeline.\n",
    "        prefix_length = generate_kwargs.pop(\"prefix_length\", 0)\n",
    "        if prefix_length > 0:\n",
    "            has_max_new_tokens = \"max_new_tokens\" in generate_kwargs or (\n",
    "                \"generation_config\" in generate_kwargs\n",
    "                and generate_kwargs[\"generation_config\"].max_new_tokens is not None\n",
    "            )\n",
    "            if not has_max_new_tokens:\n",
    "                generate_kwargs[\"max_length\"] = generate_kwargs.get(\"max_length\") or self.model.config.max_length\n",
    "                generate_kwargs[\"max_length\"] += prefix_length\n",
    "            has_min_new_tokens = \"min_new_tokens\" in generate_kwargs or (\n",
    "                \"generation_config\" in generate_kwargs\n",
    "                and generate_kwargs[\"generation_config\"].min_new_tokens is not None\n",
    "            )\n",
    "            if not has_min_new_tokens and \"min_length\" in generate_kwargs:\n",
    "                generate_kwargs[\"min_length\"] += prefix_length\n",
    "\n",
    "        # BS x SL\n",
    "        outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, return_dict_in_generate=True, **generate_kwargs)\n",
    "\n",
    "        n_layers = len(outputs.hidden_states[0][1:])\n",
    "        n_tokens = len(outputs.hidden_states[0][0][0, :, 0])\n",
    "        data = np.full((n_tokens, n_layers), None, dtype=object)         \n",
    "\n",
    "        for i, layer_hidden_state in enumerate(outputs.hidden_states[0]):\n",
    "            if i == 0:  # skip the embedding layer\n",
    "                continue\n",
    "            for j in range(n_tokens):\n",
    "                data[j, i-1] = layer_hidden_state[0, j, :].to(torch.float16).detach().cpu().numpy()\n",
    "\n",
    "        generated_sequence = outputs.sequences\n",
    "        \n",
    "        out_b = generated_sequence.shape[0]\n",
    "        if self.framework == \"pt\":\n",
    "            generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
    "        elif self.framework == \"tf\":\n",
    "            generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))\n",
    "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"prompt_text\": prompt_text, \"hidden_states\": data}\n",
    "\n",
    "    def postprocess(self, model_outputs, return_type=ReturnType.FULL_TEXT, clean_up_tokenization_spaces=True):\n",
    "        \n",
    "        # compute the raw decoded tokens in string format\n",
    "        generated_sequence = model_outputs['generated_sequence'][0,0,:]\n",
    "        tokens = [tokenizer.decode([token_id], skip_special_tokens=False, clean_up_tokenization_spaces=False) for token_id in generated_sequence]\n",
    "\n",
    "        result = super().postprocess(model_outputs, return_type, clean_up_tokenization_spaces)\n",
    "        result.append({\"tokens\": tokens})\n",
    "        result.append({\"hidden_states\": model_outputs[\"hidden_states\"]})\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input:  ['The', 'Space', 'Need', 'le', 'is', 'located', 'in', 'dow', 'nt', 'own']\n",
      "Tokenized Output:  ['The', 'Space', 'Need', 'le', 'is', 'located', 'in', 'dow', 'nt', 'own', 'Seattle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqm9ba/.conda/envs/nlp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer 1</th>\n",
       "      <th>layer 2</th>\n",
       "      <th>layer 3</th>\n",
       "      <th>layer 4</th>\n",
       "      <th>layer 5</th>\n",
       "      <th>layer 6</th>\n",
       "      <th>layer 7</th>\n",
       "      <th>layer 8</th>\n",
       "      <th>layer 9</th>\n",
       "      <th>layer 10</th>\n",
       "      <th>...</th>\n",
       "      <th>layer 23</th>\n",
       "      <th>layer 24</th>\n",
       "      <th>layer 25</th>\n",
       "      <th>layer 26</th>\n",
       "      <th>layer 27</th>\n",
       "      <th>layer 28</th>\n",
       "      <th>layer 29</th>\n",
       "      <th>layer 30</th>\n",
       "      <th>layer 31</th>\n",
       "      <th>layer 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>[0.02136, 0.03955, -0.006287, -0.1289, 0.02893...</td>\n",
       "      <td>[0.06055, -0.003418, 0.02124, -0.1816, 0.05225...</td>\n",
       "      <td>[0.0327, -0.1953, -0.03125, -0.3633, -0.1504, ...</td>\n",
       "      <td>[0.04907, -0.1768, -0.031, -0.3281, -0.2695, -...</td>\n",
       "      <td>[0.801, -0.3047, -0.05664, 0.1543, -1.055, 0.3...</td>\n",
       "      <td>[0.8555, -0.2598, -0.03467, 0.1758, -1.047, 0....</td>\n",
       "      <td>[0.824, -0.379, -0.0752, 0.3672, -0.8438, 0.61...</td>\n",
       "      <td>[1.406, -0.707, -0.03027, 0.2432, -1.484, 0.12...</td>\n",
       "      <td>[1.4375, -0.7656, 0.0249, 0.249, -1.4375, 0.15...</td>\n",
       "      <td>[1.469, -0.6094, 0.0679, 0.2695, -1.414, 0.174...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.008, -1.828, 0.9062, 0.04053, -1.75, 1.125,...</td>\n",
       "      <td>[1.047, -2.078, 1.1875, 0.1357, -1.5625, 1.055...</td>\n",
       "      <td>[1.141, -2.14, 1.32, 0.3809, -1.586, 1.016, 0....</td>\n",
       "      <td>[1.3125, -2.344, 1.625, 0.5664, -1.422, 0.953,...</td>\n",
       "      <td>[1.18, -2.312, 1.844, 0.582, -1.1875, 0.957, 0...</td>\n",
       "      <td>[0.9453, -2.172, 1.602, 0.1797, -1.219, 0.664,...</td>\n",
       "      <td>[1.445, -2.516, 2.5, 0.2832, -1.633, 0.3867, 0...</td>\n",
       "      <td>[1.016, -0.75, 0.711, -1.094, 0.2422, 0.1309, ...</td>\n",
       "      <td>[1.32, -0.879, 1.953, -0.711, -0.7734, 0.1289,...</td>\n",
       "      <td>[-0.3477, 0.3184, -0.1426, -0.0996, 0.3828, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Space</th>\n",
       "      <td>[-0.0698, 0.001419, 0.08887, -0.03125, 0.01318...</td>\n",
       "      <td>[-0.124, 0.01782, 0.1631, -0.003174, 0.05957, ...</td>\n",
       "      <td>[-0.1123, 0.1221, 0.1738, 0.05273, 0.04688, 0....</td>\n",
       "      <td>[0.01318, -0.01709, 0.2021, 0.01465, 0.11426, ...</td>\n",
       "      <td>[0.126, 0.0957, 0.2812, -0.1924, 0.1436, -0.11...</td>\n",
       "      <td>[0.1299, 0.1299, 0.5586, -0.1445, 0.1426, -0.1...</td>\n",
       "      <td>[0.2285, -0.04395, 0.629, -0.2812, 0.2754, -0....</td>\n",
       "      <td>[0.3066, 0.09424, 0.8477, 0.02734, 0.03223, -0...</td>\n",
       "      <td>[0.4336, 0.1357, 1.109, -0.1396, 0.1123, -0.21...</td>\n",
       "      <td>[0.2021, 0.3242, 1.0625, -0.1445, 0.293, -0.24...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.6055, 2.14, 3.219, -3.219, 1.875, 0.3594, -...</td>\n",
       "      <td>[1.109, 2.234, 3.922, -3.0, 3.469, -0.1699, -4...</td>\n",
       "      <td>[0.7695, 3.594, 3.344, -3.219, 3.797, -0.2734,...</td>\n",
       "      <td>[1.281, 4.375, 4.28, -3.984, 2.562, 1.273, -5....</td>\n",
       "      <td>[2.344, 5.375, 4.47, -4.094, 5.094, 0.293, -5....</td>\n",
       "      <td>[2.39, 6.906, 2.406, -2.797, 3.875, 0.547, -6....</td>\n",
       "      <td>[1.859, 6.688, 3.156, -1.414, 5.594, 0.6484, -...</td>\n",
       "      <td>[1.1875, 8.75, 3.219, -2.953, 6.062, 0.8125, -...</td>\n",
       "      <td>[1.328, 8.94, 3.422, -3.594, 6.062, 0.789, -6....</td>\n",
       "      <td>[0.797, 2.297, 0.871, -0.3906, 1.641, 0.922, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need</th>\n",
       "      <td>[-0.02087, 0.02039, 0.07715, -0.04443, -0.0556...</td>\n",
       "      <td>[-0.0381, 0.0327, 0.1582, -0.0801, -0.0791, -0...</td>\n",
       "      <td>[-0.06177, 0.003418, 0.11816, -0.08594, -0.040...</td>\n",
       "      <td>[-0.08496, 0.289, 0.3496, -0.03125, 0.0459, -0...</td>\n",
       "      <td>[0.06104, 0.3613, 0.1953, 0.1807, -0.10156, -0...</td>\n",
       "      <td>[-0.0825, 0.1719, 0.2461, 0.1816, -0.1309, -0....</td>\n",
       "      <td>[0.1719, 0.0586, 0.1865, 0.1963, 0.05176, -0.6...</td>\n",
       "      <td>[0.2461, -0.1289, -0.2207, 0.59, 0.03198, -0.6...</td>\n",
       "      <td>[0.3828, -0.0791, -0.295, 0.338, 0.2422, -0.59...</td>\n",
       "      <td>[0.42, 0.012695, -0.547, 0.7344, 0.3125, -0.91...</td>\n",
       "      <td>...</td>\n",
       "      <td>[4.344, -2.922, 0.3418, 1.141, 1.859, -0.418, ...</td>\n",
       "      <td>[3.438, -2.844, 1.844, 1.328, 1.391, -1.109, -...</td>\n",
       "      <td>[3.828, -2.188, 2.516, 2.203, 0.1406, -0.3906,...</td>\n",
       "      <td>[3.36, -3.656, 3.156, 2.75, 0.8203, -0.0586, -...</td>\n",
       "      <td>[2.75, -3.953, 4.75, 3.422, 2.703, -0.633, -0....</td>\n",
       "      <td>[1.5, -2.766, 3.031, 4.25, 2.688, -0.8047, 1.6...</td>\n",
       "      <td>[1.094, -2.672, 4.312, 5.5, 4.062, -0.5, -0.14...</td>\n",
       "      <td>[-0.4922, -2.89, 4.875, 6.438, 5.75, -1.859, 1...</td>\n",
       "      <td>[0.209, -1.805, 4.875, 6.906, 7.75, -1.852, 1....</td>\n",
       "      <td>[0.6953, 0.0598, 1.57, 1.5625, 2.36, -0.4727, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>[-0.04053, -0.02893, -0.0141, -0.02881, -0.009...</td>\n",
       "      <td>[0.0227, -0.1406, 0.02197, -0.0232, 0.10156, 0...</td>\n",
       "      <td>[0.1416, -0.00293, 0.03027, -0.042, 0.0669, -0...</td>\n",
       "      <td>[0.3398, 0.1797, -0.1426, 0.06445, 0.0947, -0....</td>\n",
       "      <td>[0.336, 0.2441, -0.1855, 0.3867, 0.00903, 0.08...</td>\n",
       "      <td>[0.2139, 0.1094, -0.5156, 0.742, -0.2178, -0.0...</td>\n",
       "      <td>[0.287, 0.1533, -0.4062, 0.5312, -0.01172, 0.3...</td>\n",
       "      <td>[0.3887, 0.2129, -0.2578, 0.543, -0.2236, -0.1...</td>\n",
       "      <td>[0.2217, -0.126, -0.6055, 0.539, -0.5234, -0.4...</td>\n",
       "      <td>[0.8516, -0.1387, -0.2031, 1.0, -0.8125, -0.37...</td>\n",
       "      <td>...</td>\n",
       "      <td>[2.734, 1.0, -2.125, 1.164, 2.469, -0.6094, -1...</td>\n",
       "      <td>[2.203, 1.508, -2.406, 1.125, 2.734, -1.719, -...</td>\n",
       "      <td>[4.188, 2.188, -3.0, -0.2852, 2.266, -2.766, -...</td>\n",
       "      <td>[4.062, 2.344, -3.422, -0.2598, 0.797, -2.062,...</td>\n",
       "      <td>[3.344, 2.156, -3.688, -0.1758, 1.8125, -2.547...</td>\n",
       "      <td>[3.672, 2.25, -3.39, 0.9883, 1.789, -3.0, -3.5...</td>\n",
       "      <td>[2.656, 2.25, -4.094, 0.578, 2.453, -2.906, -4...</td>\n",
       "      <td>[1.914, 0.9375, -5.906, 0.336, 2.547, -4.062, ...</td>\n",
       "      <td>[0.9023, 1.359, -7.188, 0.992, 4.5, -4.062, -5...</td>\n",
       "      <td>[-0.4883, 1.07, -0.9375, 0.5156, 1.242, -0.425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>[-0.02295, -0.01636, -0.01221, 0.02417, -0.044...</td>\n",
       "      <td>[-0.04468, -0.0166, 0.03345, 0.05444, -0.02856...</td>\n",
       "      <td>[-0.05957, 0.001648, 0.02539, 0.1396, -0.0625,...</td>\n",
       "      <td>[0.01538, -0.00946, -0.01514, 0.1035, -0.00818...</td>\n",
       "      <td>[0.00806, 0.1699, -0.1416, 0.04443, -0.008606,...</td>\n",
       "      <td>[0.003418, 0.1738, -0.2734, 0.1123, -0.10254, ...</td>\n",
       "      <td>[0.021, 0.166, -0.2969, 0.1245, -0.05664, 0.15...</td>\n",
       "      <td>[0.02417, 0.2734, -0.4648, 0.1836, -0.06445, 0...</td>\n",
       "      <td>[0.02039, 0.4922, -0.5273, 0.2188, -0.1982, -0...</td>\n",
       "      <td>[0.127, 0.66, -0.455, 0.3906, -0.0703, 0.08496...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.02026, -1.477, -3.188, 0.457, 3.016, 0.169...</td>\n",
       "      <td>[-0.08496, -0.633, -3.5, -0.1367, 3.844, -0.56...</td>\n",
       "      <td>[-0.4531, -0.9453, -3.484, -0.703, 6.25, -1.52...</td>\n",
       "      <td>[0.2012, -0.9062, -2.797, -0.75, 6.938, -0.023...</td>\n",
       "      <td>[-0.672, 0.1641, -4.344, -0.8516, 6.97, -0.130...</td>\n",
       "      <td>[-1.297, 0.2168, -4.75, -0.9062, 7.344, -1.219...</td>\n",
       "      <td>[-1.094, 0.7266, -6.125, -0.09375, 6.156, -0.7...</td>\n",
       "      <td>[0.01172, 0.9727, -7.344, -0.1289, 6.125, -0.0...</td>\n",
       "      <td>[-1.219, 4.22, -7.062, -0.02539, 6.344, 2.688,...</td>\n",
       "      <td>[-0.7695, 1.898, -1.016, 0.6523, 1.711, 1.094,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>located</th>\n",
       "      <td>[0.02222, -0.06152, -0.03394, -0.02588, -0.042...</td>\n",
       "      <td>[0.1138, -0.09766, 0.02087, -0.04688, -0.03113...</td>\n",
       "      <td>[0.1309, -0.2461, -0.1279, 0.001709, -0.05005,...</td>\n",
       "      <td>[0.1855, -0.1045, -0.1855, -0.084, -0.0337, -0...</td>\n",
       "      <td>[0.2422, -0.0249, -0.3906, -0.2344, -0.01904, ...</td>\n",
       "      <td>[0.254, 0.02454, -0.4336, -0.42, 0.0359, -0.06...</td>\n",
       "      <td>[0.05225, 0.11426, -0.3848, -0.7656, -0.2227, ...</td>\n",
       "      <td>[0.2197, -0.03516, -0.4082, -0.707, 0.11523, -...</td>\n",
       "      <td>[0.000977, 0.1221, -0.5312, -0.4297, -0.1484, ...</td>\n",
       "      <td>[-0.01807, 0.3906, -0.4375, -0.336, -0.293, -0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.2812, -2.219, -1.453, -4.125, 0.797, -2.031...</td>\n",
       "      <td>[1.3125, -2.812, -1.156, -3.984, 0.7344, -2.95...</td>\n",
       "      <td>[1.867, -3.39, -2.688, -4.062, 1.9375, -3.484,...</td>\n",
       "      <td>[1.609, -3.531, -2.828, -3.047, 1.719, -3.953,...</td>\n",
       "      <td>[0.8203, -3.781, -2.719, -2.156, 1.719, -5.312...</td>\n",
       "      <td>[0.2441, -1.703, -2.734, -2.219, 1.617, -4.562...</td>\n",
       "      <td>[-0.3848, -1.508, -3.125, -2.984, 1.836, -4.12...</td>\n",
       "      <td>[0.1113, 0.9766, -3.812, -4.312, 1.828, -3.25,...</td>\n",
       "      <td>[-2.14, 0.875, -4.03, -2.656, -0.539, 0.01563,...</td>\n",
       "      <td>[-1.016, 0.867, -0.535, 0.3066, 0.59, 0.4922, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>[0.03906, -0.02563, 0.00879, -0.021, -0.011475...</td>\n",
       "      <td>[0.03516, -0.004456, 0.0354, -0.008545, -0.020...</td>\n",
       "      <td>[0.05615, 0.02502, 0.02466, 0.001099, -0.02283...</td>\n",
       "      <td>[0.03638, 0.1377, -0.01636, -0.04272, -0.0796,...</td>\n",
       "      <td>[0.126, 0.1816, -0.09375, -0.1768, -0.07715, -...</td>\n",
       "      <td>[0.2695, 0.1074, -0.2012, -0.2393, -0.02222, -...</td>\n",
       "      <td>[0.209, 0.02783, -0.06836, -0.4355, -0.1885, -...</td>\n",
       "      <td>[0.1807, 0.05518, -0.2734, -0.166, -0.1138, -0...</td>\n",
       "      <td>[0.08984, 0.1426, -0.249, -0.0786, -0.04736, -...</td>\n",
       "      <td>[-0.083, 0.1611, -0.0869, 0.0293, -0.3184, -0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.574, -1.055, -2.984, -0.965, 1.375, -4.125...</td>\n",
       "      <td>[-0.6094, -2.484, -2.531, -1.578, 1.828, -3.67...</td>\n",
       "      <td>[0.4062, -2.766, -3.078, -2.36, 2.61, -6.125, ...</td>\n",
       "      <td>[1.289, -2.031, -2.344, -2.156, 3.172, -5.72, ...</td>\n",
       "      <td>[1.352, -3.719, -2.156, -2.578, 3.188, -7.375,...</td>\n",
       "      <td>[-1.703, -3.5, -3.0, -3.328, 4.0, -4.375, -4.1...</td>\n",
       "      <td>[-3.438, -2.484, -3.719, -6.22, 5.406, -5.5, -...</td>\n",
       "      <td>[-5.125, -1.727, -3.812, -6.875, 6.875, -6.844...</td>\n",
       "      <td>[-6.938, -1.359, -4.562, -5.875, 7.344, -5.688...</td>\n",
       "      <td>[-1.4375, 0.1826, -0.574, -0.5977, 1.961, -0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow</th>\n",
       "      <td>[0.10645, 0.084, 0.0884, -0.001587, -0.06885, ...</td>\n",
       "      <td>[0.1309, 0.1318, 0.1348, -0.004883, -0.11816, ...</td>\n",
       "      <td>[0.1729, 0.208, 0.1562, 0.02185, -0.1338, -0.1...</td>\n",
       "      <td>[0.1826, 0.252, 0.0923, 0.01117, -0.0957, -0.1...</td>\n",
       "      <td>[0.1504, 0.2715, 0.0315, -0.02979, -0.0547, -0...</td>\n",
       "      <td>[0.1289, 0.1748, 0.08105, 0.1855, -0.3633, -0....</td>\n",
       "      <td>[-0.00952, -0.012695, 0.2656, 0.08496, -0.4883...</td>\n",
       "      <td>[0.3672, -0.4414, 0.1797, 0.1328, -0.617, 0.23...</td>\n",
       "      <td>[0.3965, -0.5117, 0.5156, 0.5, -0.4531, 0.3164...</td>\n",
       "      <td>[0.5117, -0.7344, 0.3965, 0.8203, -0.8516, 0.2...</td>\n",
       "      <td>...</td>\n",
       "      <td>[2.781, -1.359, -0.8867, 0.1445, -3.281, 0.077...</td>\n",
       "      <td>[1.352, -1.297, 0.633, 0.672, -3.203, -0.2354,...</td>\n",
       "      <td>[1.0625, -1.492, 0.4707, 1.094, -3.516, -1.711...</td>\n",
       "      <td>[0.703, -2.75, 1.477, 2.125, -3.672, -2.156, 2...</td>\n",
       "      <td>[1.375, -1.484, 1.1875, 4.125, -3.719, -3.36, ...</td>\n",
       "      <td>[1.594, -1.0625, -0.125, 6.438, -4.22, -2.39, ...</td>\n",
       "      <td>[3.188, -3.047, -1.055, 7.906, -3.438, -1.602,...</td>\n",
       "      <td>[2.906, -2.86, -1.117, 9.19, -1.844, -0.3594, ...</td>\n",
       "      <td>[3.438, -4.47, -1.656, 8.875, -2.703, 0.7812, ...</td>\n",
       "      <td>[0.7617, -0.6953, -0.6016, 3.125, -1.6875, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt</th>\n",
       "      <td>[-0.05322, 0.02124, -0.004913, -0.02393, -0.02...</td>\n",
       "      <td>[0.0669, -0.0635, 0.04126, 0.0371, -0.1289, -0...</td>\n",
       "      <td>[0.09863, -0.04395, -0.04102, -0.02295, -0.171...</td>\n",
       "      <td>[0.0703, -0.0664, -0.125, -0.09814, -0.0913, -...</td>\n",
       "      <td>[0.06396, -0.0659, -0.1504, -0.10596, -0.08105...</td>\n",
       "      <td>[0.05615, -0.0957, -0.1836, -0.0498, -0.10156,...</td>\n",
       "      <td>[0.01978, -0.1221, -0.10645, -0.1172, -0.1504,...</td>\n",
       "      <td>[0.1543, -0.1875, -0.2109, -0.1289, -0.08057, ...</td>\n",
       "      <td>[0.1611, -0.2139, -0.09033, -0.1885, -0.009766...</td>\n",
       "      <td>[0.1729, 0.02051, -0.2295, -0.1738, -0.1045, -...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.3125, 0.04883, -0.875, -0.699, -2.469, -0....</td>\n",
       "      <td>[-0.9805, 0.05273, -0.2441, -0.371, -2.594, -0...</td>\n",
       "      <td>[-0.461, 0.293, 0.3242, -0.1157, -2.375, -0.32...</td>\n",
       "      <td>[0.0703, 0.961, -2.328, -2.719, -3.688, -2.64,...</td>\n",
       "      <td>[0.1309, 1.18, -2.016, -2.781, -3.703, -2.734,...</td>\n",
       "      <td>[0.4766, 0.2988, -1.6875, -1.977, -3.438, -1.6...</td>\n",
       "      <td>[0.6406, -0.007812, -1.641, -2.531, -2.672, -1...</td>\n",
       "      <td>[0.6875, -0.6094, -1.203, -0.5312, -2.484, 1.9...</td>\n",
       "      <td>[2.469, 0.05078, -0.3672, -1.023, -3.281, 2.95...</td>\n",
       "      <td>[1.219, -0.2617, -0.949, 0.965, 1.5625, 1.109,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>own</th>\n",
       "      <td>[-0.03418, 0.05103, -0.01563, -0.0547, 0.00073...</td>\n",
       "      <td>[-0.0354, 0.083, 0.003174, 0.08154, -0.1855, -...</td>\n",
       "      <td>[-0.3066, 0.06836, 0.02124, 0.2363, -0.3984, -...</td>\n",
       "      <td>[-0.3965, 0.0718, 0.03613, 0.2188, -0.547, -0....</td>\n",
       "      <td>[-0.332, 0.0498, -0.0094, 0.166, -0.551, -0.13...</td>\n",
       "      <td>[-0.295, 0.04126, 0.0581, 0.4258, -0.75, -0.08...</td>\n",
       "      <td>[-0.6484, -0.03735, 0.084, 0.1943, -1.047, 0.0...</td>\n",
       "      <td>[-0.3516, -0.03418, 0.01123, 0.1553, -1.219, 0...</td>\n",
       "      <td>[-0.336, -0.6484, 0.2168, 0.167, -0.8203, 0.18...</td>\n",
       "      <td>[0.0781, -0.4375, -0.006836, 0.254, -0.9844, -...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.1172, -1.523, -2.61, -2.594, -0.461, -4.59...</td>\n",
       "      <td>[-0.3242, -2.969, -2.281, -3.328, -0.3203, -4....</td>\n",
       "      <td>[-0.664, -3.172, -2.828, -3.922, 0.758, -6.562...</td>\n",
       "      <td>[-0.10547, -2.266, -1.953, -4.125, 0.8906, -6....</td>\n",
       "      <td>[-0.1289, -4.188, -3.094, -4.844, 2.062, -9.25...</td>\n",
       "      <td>[-2.422, -4.438, -3.5, -4.406, 2.844, -5.344, ...</td>\n",
       "      <td>[-3.969, -4.062, -4.53, -6.25, 3.719, -7.094, ...</td>\n",
       "      <td>[-5.53, -3.672, -3.594, -4.906, 4.75, -9.06, -...</td>\n",
       "      <td>[-7.875, -3.094, -3.781, -4.688, 4.906, -7.47,...</td>\n",
       "      <td>[-1.25, -0.295, -0.633, -0.4219, 1.602, -1.187...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   layer 1  \\\n",
       "The      [0.02136, 0.03955, -0.006287, -0.1289, 0.02893...   \n",
       "Space    [-0.0698, 0.001419, 0.08887, -0.03125, 0.01318...   \n",
       "Need     [-0.02087, 0.02039, 0.07715, -0.04443, -0.0556...   \n",
       "le       [-0.04053, -0.02893, -0.0141, -0.02881, -0.009...   \n",
       "is       [-0.02295, -0.01636, -0.01221, 0.02417, -0.044...   \n",
       "located  [0.02222, -0.06152, -0.03394, -0.02588, -0.042...   \n",
       "in       [0.03906, -0.02563, 0.00879, -0.021, -0.011475...   \n",
       "dow      [0.10645, 0.084, 0.0884, -0.001587, -0.06885, ...   \n",
       "nt       [-0.05322, 0.02124, -0.004913, -0.02393, -0.02...   \n",
       "own      [-0.03418, 0.05103, -0.01563, -0.0547, 0.00073...   \n",
       "\n",
       "                                                   layer 2  \\\n",
       "The      [0.06055, -0.003418, 0.02124, -0.1816, 0.05225...   \n",
       "Space    [-0.124, 0.01782, 0.1631, -0.003174, 0.05957, ...   \n",
       "Need     [-0.0381, 0.0327, 0.1582, -0.0801, -0.0791, -0...   \n",
       "le       [0.0227, -0.1406, 0.02197, -0.0232, 0.10156, 0...   \n",
       "is       [-0.04468, -0.0166, 0.03345, 0.05444, -0.02856...   \n",
       "located  [0.1138, -0.09766, 0.02087, -0.04688, -0.03113...   \n",
       "in       [0.03516, -0.004456, 0.0354, -0.008545, -0.020...   \n",
       "dow      [0.1309, 0.1318, 0.1348, -0.004883, -0.11816, ...   \n",
       "nt       [0.0669, -0.0635, 0.04126, 0.0371, -0.1289, -0...   \n",
       "own      [-0.0354, 0.083, 0.003174, 0.08154, -0.1855, -...   \n",
       "\n",
       "                                                   layer 3  \\\n",
       "The      [0.0327, -0.1953, -0.03125, -0.3633, -0.1504, ...   \n",
       "Space    [-0.1123, 0.1221, 0.1738, 0.05273, 0.04688, 0....   \n",
       "Need     [-0.06177, 0.003418, 0.11816, -0.08594, -0.040...   \n",
       "le       [0.1416, -0.00293, 0.03027, -0.042, 0.0669, -0...   \n",
       "is       [-0.05957, 0.001648, 0.02539, 0.1396, -0.0625,...   \n",
       "located  [0.1309, -0.2461, -0.1279, 0.001709, -0.05005,...   \n",
       "in       [0.05615, 0.02502, 0.02466, 0.001099, -0.02283...   \n",
       "dow      [0.1729, 0.208, 0.1562, 0.02185, -0.1338, -0.1...   \n",
       "nt       [0.09863, -0.04395, -0.04102, -0.02295, -0.171...   \n",
       "own      [-0.3066, 0.06836, 0.02124, 0.2363, -0.3984, -...   \n",
       "\n",
       "                                                   layer 4  \\\n",
       "The      [0.04907, -0.1768, -0.031, -0.3281, -0.2695, -...   \n",
       "Space    [0.01318, -0.01709, 0.2021, 0.01465, 0.11426, ...   \n",
       "Need     [-0.08496, 0.289, 0.3496, -0.03125, 0.0459, -0...   \n",
       "le       [0.3398, 0.1797, -0.1426, 0.06445, 0.0947, -0....   \n",
       "is       [0.01538, -0.00946, -0.01514, 0.1035, -0.00818...   \n",
       "located  [0.1855, -0.1045, -0.1855, -0.084, -0.0337, -0...   \n",
       "in       [0.03638, 0.1377, -0.01636, -0.04272, -0.0796,...   \n",
       "dow      [0.1826, 0.252, 0.0923, 0.01117, -0.0957, -0.1...   \n",
       "nt       [0.0703, -0.0664, -0.125, -0.09814, -0.0913, -...   \n",
       "own      [-0.3965, 0.0718, 0.03613, 0.2188, -0.547, -0....   \n",
       "\n",
       "                                                   layer 5  \\\n",
       "The      [0.801, -0.3047, -0.05664, 0.1543, -1.055, 0.3...   \n",
       "Space    [0.126, 0.0957, 0.2812, -0.1924, 0.1436, -0.11...   \n",
       "Need     [0.06104, 0.3613, 0.1953, 0.1807, -0.10156, -0...   \n",
       "le       [0.336, 0.2441, -0.1855, 0.3867, 0.00903, 0.08...   \n",
       "is       [0.00806, 0.1699, -0.1416, 0.04443, -0.008606,...   \n",
       "located  [0.2422, -0.0249, -0.3906, -0.2344, -0.01904, ...   \n",
       "in       [0.126, 0.1816, -0.09375, -0.1768, -0.07715, -...   \n",
       "dow      [0.1504, 0.2715, 0.0315, -0.02979, -0.0547, -0...   \n",
       "nt       [0.06396, -0.0659, -0.1504, -0.10596, -0.08105...   \n",
       "own      [-0.332, 0.0498, -0.0094, 0.166, -0.551, -0.13...   \n",
       "\n",
       "                                                   layer 6  \\\n",
       "The      [0.8555, -0.2598, -0.03467, 0.1758, -1.047, 0....   \n",
       "Space    [0.1299, 0.1299, 0.5586, -0.1445, 0.1426, -0.1...   \n",
       "Need     [-0.0825, 0.1719, 0.2461, 0.1816, -0.1309, -0....   \n",
       "le       [0.2139, 0.1094, -0.5156, 0.742, -0.2178, -0.0...   \n",
       "is       [0.003418, 0.1738, -0.2734, 0.1123, -0.10254, ...   \n",
       "located  [0.254, 0.02454, -0.4336, -0.42, 0.0359, -0.06...   \n",
       "in       [0.2695, 0.1074, -0.2012, -0.2393, -0.02222, -...   \n",
       "dow      [0.1289, 0.1748, 0.08105, 0.1855, -0.3633, -0....   \n",
       "nt       [0.05615, -0.0957, -0.1836, -0.0498, -0.10156,...   \n",
       "own      [-0.295, 0.04126, 0.0581, 0.4258, -0.75, -0.08...   \n",
       "\n",
       "                                                   layer 7  \\\n",
       "The      [0.824, -0.379, -0.0752, 0.3672, -0.8438, 0.61...   \n",
       "Space    [0.2285, -0.04395, 0.629, -0.2812, 0.2754, -0....   \n",
       "Need     [0.1719, 0.0586, 0.1865, 0.1963, 0.05176, -0.6...   \n",
       "le       [0.287, 0.1533, -0.4062, 0.5312, -0.01172, 0.3...   \n",
       "is       [0.021, 0.166, -0.2969, 0.1245, -0.05664, 0.15...   \n",
       "located  [0.05225, 0.11426, -0.3848, -0.7656, -0.2227, ...   \n",
       "in       [0.209, 0.02783, -0.06836, -0.4355, -0.1885, -...   \n",
       "dow      [-0.00952, -0.012695, 0.2656, 0.08496, -0.4883...   \n",
       "nt       [0.01978, -0.1221, -0.10645, -0.1172, -0.1504,...   \n",
       "own      [-0.6484, -0.03735, 0.084, 0.1943, -1.047, 0.0...   \n",
       "\n",
       "                                                   layer 8  \\\n",
       "The      [1.406, -0.707, -0.03027, 0.2432, -1.484, 0.12...   \n",
       "Space    [0.3066, 0.09424, 0.8477, 0.02734, 0.03223, -0...   \n",
       "Need     [0.2461, -0.1289, -0.2207, 0.59, 0.03198, -0.6...   \n",
       "le       [0.3887, 0.2129, -0.2578, 0.543, -0.2236, -0.1...   \n",
       "is       [0.02417, 0.2734, -0.4648, 0.1836, -0.06445, 0...   \n",
       "located  [0.2197, -0.03516, -0.4082, -0.707, 0.11523, -...   \n",
       "in       [0.1807, 0.05518, -0.2734, -0.166, -0.1138, -0...   \n",
       "dow      [0.3672, -0.4414, 0.1797, 0.1328, -0.617, 0.23...   \n",
       "nt       [0.1543, -0.1875, -0.2109, -0.1289, -0.08057, ...   \n",
       "own      [-0.3516, -0.03418, 0.01123, 0.1553, -1.219, 0...   \n",
       "\n",
       "                                                   layer 9  \\\n",
       "The      [1.4375, -0.7656, 0.0249, 0.249, -1.4375, 0.15...   \n",
       "Space    [0.4336, 0.1357, 1.109, -0.1396, 0.1123, -0.21...   \n",
       "Need     [0.3828, -0.0791, -0.295, 0.338, 0.2422, -0.59...   \n",
       "le       [0.2217, -0.126, -0.6055, 0.539, -0.5234, -0.4...   \n",
       "is       [0.02039, 0.4922, -0.5273, 0.2188, -0.1982, -0...   \n",
       "located  [0.000977, 0.1221, -0.5312, -0.4297, -0.1484, ...   \n",
       "in       [0.08984, 0.1426, -0.249, -0.0786, -0.04736, -...   \n",
       "dow      [0.3965, -0.5117, 0.5156, 0.5, -0.4531, 0.3164...   \n",
       "nt       [0.1611, -0.2139, -0.09033, -0.1885, -0.009766...   \n",
       "own      [-0.336, -0.6484, 0.2168, 0.167, -0.8203, 0.18...   \n",
       "\n",
       "                                                  layer 10  ...  \\\n",
       "The      [1.469, -0.6094, 0.0679, 0.2695, -1.414, 0.174...  ...   \n",
       "Space    [0.2021, 0.3242, 1.0625, -0.1445, 0.293, -0.24...  ...   \n",
       "Need     [0.42, 0.012695, -0.547, 0.7344, 0.3125, -0.91...  ...   \n",
       "le       [0.8516, -0.1387, -0.2031, 1.0, -0.8125, -0.37...  ...   \n",
       "is       [0.127, 0.66, -0.455, 0.3906, -0.0703, 0.08496...  ...   \n",
       "located  [-0.01807, 0.3906, -0.4375, -0.336, -0.293, -0...  ...   \n",
       "in       [-0.083, 0.1611, -0.0869, 0.0293, -0.3184, -0....  ...   \n",
       "dow      [0.5117, -0.7344, 0.3965, 0.8203, -0.8516, 0.2...  ...   \n",
       "nt       [0.1729, 0.02051, -0.2295, -0.1738, -0.1045, -...  ...   \n",
       "own      [0.0781, -0.4375, -0.006836, 0.254, -0.9844, -...  ...   \n",
       "\n",
       "                                                  layer 23  \\\n",
       "The      [1.008, -1.828, 0.9062, 0.04053, -1.75, 1.125,...   \n",
       "Space    [0.6055, 2.14, 3.219, -3.219, 1.875, 0.3594, -...   \n",
       "Need     [4.344, -2.922, 0.3418, 1.141, 1.859, -0.418, ...   \n",
       "le       [2.734, 1.0, -2.125, 1.164, 2.469, -0.6094, -1...   \n",
       "is       [-0.02026, -1.477, -3.188, 0.457, 3.016, 0.169...   \n",
       "located  [0.2812, -2.219, -1.453, -4.125, 0.797, -2.031...   \n",
       "in       [-0.574, -1.055, -2.984, -0.965, 1.375, -4.125...   \n",
       "dow      [2.781, -1.359, -0.8867, 0.1445, -3.281, 0.077...   \n",
       "nt       [-1.3125, 0.04883, -0.875, -0.699, -2.469, -0....   \n",
       "own      [-0.1172, -1.523, -2.61, -2.594, -0.461, -4.59...   \n",
       "\n",
       "                                                  layer 24  \\\n",
       "The      [1.047, -2.078, 1.1875, 0.1357, -1.5625, 1.055...   \n",
       "Space    [1.109, 2.234, 3.922, -3.0, 3.469, -0.1699, -4...   \n",
       "Need     [3.438, -2.844, 1.844, 1.328, 1.391, -1.109, -...   \n",
       "le       [2.203, 1.508, -2.406, 1.125, 2.734, -1.719, -...   \n",
       "is       [-0.08496, -0.633, -3.5, -0.1367, 3.844, -0.56...   \n",
       "located  [1.3125, -2.812, -1.156, -3.984, 0.7344, -2.95...   \n",
       "in       [-0.6094, -2.484, -2.531, -1.578, 1.828, -3.67...   \n",
       "dow      [1.352, -1.297, 0.633, 0.672, -3.203, -0.2354,...   \n",
       "nt       [-0.9805, 0.05273, -0.2441, -0.371, -2.594, -0...   \n",
       "own      [-0.3242, -2.969, -2.281, -3.328, -0.3203, -4....   \n",
       "\n",
       "                                                  layer 25  \\\n",
       "The      [1.141, -2.14, 1.32, 0.3809, -1.586, 1.016, 0....   \n",
       "Space    [0.7695, 3.594, 3.344, -3.219, 3.797, -0.2734,...   \n",
       "Need     [3.828, -2.188, 2.516, 2.203, 0.1406, -0.3906,...   \n",
       "le       [4.188, 2.188, -3.0, -0.2852, 2.266, -2.766, -...   \n",
       "is       [-0.4531, -0.9453, -3.484, -0.703, 6.25, -1.52...   \n",
       "located  [1.867, -3.39, -2.688, -4.062, 1.9375, -3.484,...   \n",
       "in       [0.4062, -2.766, -3.078, -2.36, 2.61, -6.125, ...   \n",
       "dow      [1.0625, -1.492, 0.4707, 1.094, -3.516, -1.711...   \n",
       "nt       [-0.461, 0.293, 0.3242, -0.1157, -2.375, -0.32...   \n",
       "own      [-0.664, -3.172, -2.828, -3.922, 0.758, -6.562...   \n",
       "\n",
       "                                                  layer 26  \\\n",
       "The      [1.3125, -2.344, 1.625, 0.5664, -1.422, 0.953,...   \n",
       "Space    [1.281, 4.375, 4.28, -3.984, 2.562, 1.273, -5....   \n",
       "Need     [3.36, -3.656, 3.156, 2.75, 0.8203, -0.0586, -...   \n",
       "le       [4.062, 2.344, -3.422, -0.2598, 0.797, -2.062,...   \n",
       "is       [0.2012, -0.9062, -2.797, -0.75, 6.938, -0.023...   \n",
       "located  [1.609, -3.531, -2.828, -3.047, 1.719, -3.953,...   \n",
       "in       [1.289, -2.031, -2.344, -2.156, 3.172, -5.72, ...   \n",
       "dow      [0.703, -2.75, 1.477, 2.125, -3.672, -2.156, 2...   \n",
       "nt       [0.0703, 0.961, -2.328, -2.719, -3.688, -2.64,...   \n",
       "own      [-0.10547, -2.266, -1.953, -4.125, 0.8906, -6....   \n",
       "\n",
       "                                                  layer 27  \\\n",
       "The      [1.18, -2.312, 1.844, 0.582, -1.1875, 0.957, 0...   \n",
       "Space    [2.344, 5.375, 4.47, -4.094, 5.094, 0.293, -5....   \n",
       "Need     [2.75, -3.953, 4.75, 3.422, 2.703, -0.633, -0....   \n",
       "le       [3.344, 2.156, -3.688, -0.1758, 1.8125, -2.547...   \n",
       "is       [-0.672, 0.1641, -4.344, -0.8516, 6.97, -0.130...   \n",
       "located  [0.8203, -3.781, -2.719, -2.156, 1.719, -5.312...   \n",
       "in       [1.352, -3.719, -2.156, -2.578, 3.188, -7.375,...   \n",
       "dow      [1.375, -1.484, 1.1875, 4.125, -3.719, -3.36, ...   \n",
       "nt       [0.1309, 1.18, -2.016, -2.781, -3.703, -2.734,...   \n",
       "own      [-0.1289, -4.188, -3.094, -4.844, 2.062, -9.25...   \n",
       "\n",
       "                                                  layer 28  \\\n",
       "The      [0.9453, -2.172, 1.602, 0.1797, -1.219, 0.664,...   \n",
       "Space    [2.39, 6.906, 2.406, -2.797, 3.875, 0.547, -6....   \n",
       "Need     [1.5, -2.766, 3.031, 4.25, 2.688, -0.8047, 1.6...   \n",
       "le       [3.672, 2.25, -3.39, 0.9883, 1.789, -3.0, -3.5...   \n",
       "is       [-1.297, 0.2168, -4.75, -0.9062, 7.344, -1.219...   \n",
       "located  [0.2441, -1.703, -2.734, -2.219, 1.617, -4.562...   \n",
       "in       [-1.703, -3.5, -3.0, -3.328, 4.0, -4.375, -4.1...   \n",
       "dow      [1.594, -1.0625, -0.125, 6.438, -4.22, -2.39, ...   \n",
       "nt       [0.4766, 0.2988, -1.6875, -1.977, -3.438, -1.6...   \n",
       "own      [-2.422, -4.438, -3.5, -4.406, 2.844, -5.344, ...   \n",
       "\n",
       "                                                  layer 29  \\\n",
       "The      [1.445, -2.516, 2.5, 0.2832, -1.633, 0.3867, 0...   \n",
       "Space    [1.859, 6.688, 3.156, -1.414, 5.594, 0.6484, -...   \n",
       "Need     [1.094, -2.672, 4.312, 5.5, 4.062, -0.5, -0.14...   \n",
       "le       [2.656, 2.25, -4.094, 0.578, 2.453, -2.906, -4...   \n",
       "is       [-1.094, 0.7266, -6.125, -0.09375, 6.156, -0.7...   \n",
       "located  [-0.3848, -1.508, -3.125, -2.984, 1.836, -4.12...   \n",
       "in       [-3.438, -2.484, -3.719, -6.22, 5.406, -5.5, -...   \n",
       "dow      [3.188, -3.047, -1.055, 7.906, -3.438, -1.602,...   \n",
       "nt       [0.6406, -0.007812, -1.641, -2.531, -2.672, -1...   \n",
       "own      [-3.969, -4.062, -4.53, -6.25, 3.719, -7.094, ...   \n",
       "\n",
       "                                                  layer 30  \\\n",
       "The      [1.016, -0.75, 0.711, -1.094, 0.2422, 0.1309, ...   \n",
       "Space    [1.1875, 8.75, 3.219, -2.953, 6.062, 0.8125, -...   \n",
       "Need     [-0.4922, -2.89, 4.875, 6.438, 5.75, -1.859, 1...   \n",
       "le       [1.914, 0.9375, -5.906, 0.336, 2.547, -4.062, ...   \n",
       "is       [0.01172, 0.9727, -7.344, -0.1289, 6.125, -0.0...   \n",
       "located  [0.1113, 0.9766, -3.812, -4.312, 1.828, -3.25,...   \n",
       "in       [-5.125, -1.727, -3.812, -6.875, 6.875, -6.844...   \n",
       "dow      [2.906, -2.86, -1.117, 9.19, -1.844, -0.3594, ...   \n",
       "nt       [0.6875, -0.6094, -1.203, -0.5312, -2.484, 1.9...   \n",
       "own      [-5.53, -3.672, -3.594, -4.906, 4.75, -9.06, -...   \n",
       "\n",
       "                                                  layer 31  \\\n",
       "The      [1.32, -0.879, 1.953, -0.711, -0.7734, 0.1289,...   \n",
       "Space    [1.328, 8.94, 3.422, -3.594, 6.062, 0.789, -6....   \n",
       "Need     [0.209, -1.805, 4.875, 6.906, 7.75, -1.852, 1....   \n",
       "le       [0.9023, 1.359, -7.188, 0.992, 4.5, -4.062, -5...   \n",
       "is       [-1.219, 4.22, -7.062, -0.02539, 6.344, 2.688,...   \n",
       "located  [-2.14, 0.875, -4.03, -2.656, -0.539, 0.01563,...   \n",
       "in       [-6.938, -1.359, -4.562, -5.875, 7.344, -5.688...   \n",
       "dow      [3.438, -4.47, -1.656, 8.875, -2.703, 0.7812, ...   \n",
       "nt       [2.469, 0.05078, -0.3672, -1.023, -3.281, 2.95...   \n",
       "own      [-7.875, -3.094, -3.781, -4.688, 4.906, -7.47,...   \n",
       "\n",
       "                                                  layer 32  \n",
       "The      [-0.3477, 0.3184, -0.1426, -0.0996, 0.3828, -0...  \n",
       "Space    [0.797, 2.297, 0.871, -0.3906, 1.641, 0.922, -...  \n",
       "Need     [0.6953, 0.0598, 1.57, 1.5625, 2.36, -0.4727, ...  \n",
       "le       [-0.4883, 1.07, -0.9375, 0.5156, 1.242, -0.425...  \n",
       "is       [-0.7695, 1.898, -1.016, 0.6523, 1.711, 1.094,...  \n",
       "located  [-1.016, 0.867, -0.535, 0.3066, 0.59, 0.4922, ...  \n",
       "in       [-1.4375, 0.1826, -0.574, -0.5977, 1.961, -0.7...  \n",
       "dow      [0.7617, -0.6953, -0.6016, 3.125, -1.6875, 1.0...  \n",
       "nt       [1.219, -0.2617, -0.949, 0.965, 1.5625, 1.109,...  \n",
       "own      [-1.25, -0.295, -0.633, -0.4219, 1.602, -1.187...  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "messages = \"The Space Needle is located in downtown\"\n",
    "max_new_tokens = 1\n",
    "\n",
    "pipe = CustomPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"output_hidden_states\": True,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "\n",
    "tokenized_input = output[1]['tokens'][:-max_new_tokens]\n",
    "generated_text = output[0]['generated_text']\n",
    "tokenized_output = output[1]['tokens']\n",
    "hidden_states = output[2][\"hidden_states\"]\n",
    "\n",
    "print(\"Tokenized Input: \", tokenized_input)\n",
    "print(\"Tokenized Output: \", tokenized_output)\n",
    "\n",
    "# visualize the hidden states. This data will be used for the Bayesian Network\n",
    "df = pd.DataFrame(hidden_states)\n",
    "df.columns = ['layer {}'.format(i+1) for i in range(df.shape[1])]\n",
    "df.index = [tokenized_input]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from numpy.random import normal\n",
    "\n",
    "\n",
    "data = hidden_states.flatten()\n",
    "\n",
    "for i, inner_list in enumerate(data):\n",
    "    # Fit a normal distribution to the data\n",
    "    mu, sigma = stats.norm.fit(inner_list)\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.hist(inner_list, bins=20, density=True, alpha=0.6, color='g')\n",
    "\n",
    "    # Plot the PDF of the fitted normal distribution\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = stats.norm.pdf(x, mu, sigma)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "    plt.title(f'Histogram of Inner List {i+1} with Fitted Normal Distribution')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Parameters for the fitted normal distribution of Inner List {i+1}:\")\n",
    "    print(f\"Mean (mu) = {mu}\")\n",
    "    print(f\"Standard deviation (sigma) = {sigma}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Estimation using Bayesian Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# define BN\n",
    "network = [('A', 'C'), ('B', 'C')]\n",
    "model = BayesianNetwork(network)\n",
    "G = nx.DiGraph(network)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=2000, font_size=15, font_color='darkred', arrowstyle='-|>', arrowsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  0  0  1\n",
       "1  0  1  1\n",
       "2  1  0  0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data for parameter estimation\n",
    "data = pd.DataFrame(data={'A': [0, 0, 1], 'B': [0, 1, 0], 'C': [1, 1, 0]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "| A(0) | 0.666667 |\n",
      "+------+----------+\n",
      "| A(1) | 0.333333 |\n",
      "+------+----------+\n",
      "+------+------+------+------+------+\n",
      "| A    | A(0) | A(0) | A(1) | A(1) |\n",
      "+------+------+------+------+------+\n",
      "| B    | B(0) | B(1) | B(0) | B(1) |\n",
      "+------+------+------+------+------+\n",
      "| C(0) | 0.0  | 0.0  | 1.0  | 0.5  |\n",
      "+------+------+------+------+------+\n",
      "| C(1) | 1.0  | 1.0  | 0.0  | 0.5  |\n",
      "+------+------+------+------+------+\n"
     ]
    }
   ],
   "source": [
    "estimator = MaximumLikelihoodEstimator(model, data)\n",
    "cpd_A = estimator.estimate_cpd('A')\n",
    "print(cpd_A)\n",
    "cpd_C = estimator.estimate_cpd('C')\n",
    "print(cpd_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+--------------------+\n",
      "| A    | A(0) | A(0) | A(1) | A(1)               |\n",
      "+------+------+------+------+--------------------+\n",
      "| B    | B(0) | B(1) | B(0) | B(1)               |\n",
      "+------+------+------+------+--------------------+\n",
      "| C(0) | 0.25 | 0.25 | 0.5  | 0.3333333333333333 |\n",
      "+------+------+------+------+--------------------+\n",
      "| C(1) | 0.75 | 0.75 | 0.5  | 0.6666666666666666 |\n",
      "+------+------+------+------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "estimator = BayesianEstimator(model, data)\n",
    "cpd_C = estimator.estimate_cpd('C', prior_type=\"dirichlet\",\n",
    "                               pseudo_counts=[[1, 1, 1, 1],\n",
    "                                              [2, 2, 2, 2]])\n",
    "print(cpd_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
